1
==


- Particular case of Conditional linear Gaussian (CLG):
  A commonly used type of hybrid Bayesian network is the conditional linear Gaussian (CLG) model [6,12,13]. In CLG models, the distribution of a continuous variable is a linear Gaussian function of its continuous parents. One limitation of CLG models is that discrete nodes cannot have continuous parents. Lerner et al. [16] introduces augmented CLG networks where discrete variables may depend on continuous parents. Several algorithms for performing approximate inference in such networks have been developed (see, e.g., [2,10,15,16,21]).
  CLG models cannot accomodate discrete nodes with continuous parents because of the as- sumption that the joint distribution is a mixture of Gaussians.

2: Challenges
==
- The complexity of these algorithms is so high that present significant challenges to perfom
- Discretization of continuous distributions can allow approximate inference in a hybrid Bayesian network without limitations on relationships among continuous and discrete variables. Dis- cretization of continuous chance variables is equivalent to approximating a probability density function (PDF) with mixtures of uniform distributions. Discretization with a small number of states can lead to poor accuracy, while discretization with a large number of states can lead to excessive computational effort.
- However, several factors make inference in hybrid models extremely hard. First, linear or nonlinear deterministic relations may exist in the models. Second, the models may con- tain arbitrary probability distributions. Third, the orderings among the discrete and continu- ous variables may be arbitrary.

3: Probability
==
- In discrete variable it uses Joint Density Function but in continuous They used a probability density function (pdf), or density of a continuous random variable, is a function that describes the relative likelihood for this random variable to take on a given value. The probability of the random variable falling within a particular range of values is given by the integral of this variable’s density over that range—that is, it is given by the area under the density function but above the horizontal axis and between the lowest and greatest values of the range.
- Heskes and Zoeter (2003) applied a generalized belief propagation to approximate inference in HBNs\[
mean: E[x], covariance: E[(x-E[x])(y-E[y])]
\]
- 
- Schrempf and Hanebeck, 2004 considered that using  first two moments is a drawback

4.1: Deterministic 
================
- Shenoy and Cobb using relationships derived from joint cumulative distribution functions (CDF’s). This allows MTE potentials to be used for inference in any CLG model, as well as other models that have conditionally deterministic variables but do not fit the CLG restrictions, such as those containing discrete nodes with continuous parents.
- General formulations of MTE potentials which approximate the normal probability density function (PDF) exist (Cobb and Shenoy, 2003); however, these formulations cannot be used to model a conditional distribution where the variance of a variable given values of its continuous parents is zero.
- When relationships between continuous vari- ables are deterministic, the joint PDF does not exist. We can express a conditional probability mass function as a degenerate function. If Y is a deterministic relationship of variables in X, i.e. y = g(x), the conditional mass function (CMF) for {Y | x} is defined as: (function)
where 1{A} is the indicator of the event A, i.e. 1{A} = 1 if A occurs and 0 otherwise. Graph-
- a deterministic algorithm is an algorithm which, given a particular input, will always produce the same output, with the underlying machine always passing through the same sequence of states

4.2: MTE
=========
- Any probability density function (PDF) can be approximated with an MTE potential, which can always be marginalized in a closed form.
- shenoy and cobb int their paper presents MTE potentials that approximate an arbitrary normal PDF with any mean and a positive variance.
	The main goal of this paper is to describe an implementation of MTE potentials in hybrid Bayesian networks where continuous distributions are conditional linear Gaussian distributions
- Assuming that the joint density exists, MTE potentials can be used for inference in hybrid Bayesian networks that do not fit the restrictive assumptions of the conditional linear Gaussian (CLG) model, such as networks containing discrete nodes with continuous parents.
- Any continuous PDF can be approximated by an MTE potential
- Figure: 4-piece, 1-term MTE approximation overlayed on the standard normal distribution
The PDF for the normal distribution is
fX (x) = √ 1 exp{ − 1 􏰁 x − μ 􏰂2 }. (6) 2πσ 2σ
A general formulation for a 4-piece, 1-term normalized MTE potential which approximates a normal PDF is as follows:
 −0.017203 + 0.930964 exp{1.27(x−μ)} if μ − 3σ ≤ x < μ − σ σ σ σ
￼￼￼￼￼￼￼ 0.442208 − 0.038452 exp{ − 1.64(x−μ)} if μ − σ ≤ x < μ φ(x)=σσ σ (7)
￼￼￼ 0.442208 − 0.038452 exp{1.64( x−μ )} if μ ≤ x < μ + σ σ σ σ
￼￼￼ −0.017203 + 0.930964 exp{ − 1.27(x−μ)} if μ + σ ≤ x < μ + 3σ. σσσ


4.3: MoGs
=========
- this paper is to describe a method for exact inference in general hybrid Bayesian networks (BNs) (with a mixture of discrete and continuous chance variables). Our method consists of approximating general hybrid Bayesian networks by a mixture of Gaussians (MoG) BNs.
Variance
- There exists a fast algorithm by Lauritzen-Jensen (LJ) (Junction Tree ALgorithm) for making exact inferences in MoG Bayesian networks, and there exists a commercial implementation of this algorithm.
	Limitations:
		- COmmercial software (mplemented in Hugin,)
		- All continuous chance variables must have conditional linear Gaussian distributions
		- Discrete chance nodes cannot have continuous parents
- The methods described in this paper will enable us to use the LJ algorithm for a bigger class of hybrid Bayesian networks. This includes networks with continuous chance nodes with non-Gaussian distributions, networks with no restrictions on the topology of discrete and continuous variables, networks with conditionally deterministic variables that are a nonlinear function of their continuous parents, and networks with continuous chance variables whose variances are functions of their parents.
- It is well known that mixtures of Gaussians can approximate any probability distribution [Titterington et al. 1985]

5: Hybrid Loopy Belief Propagation(HLBP)
=========================================
- The main idea is to represent the LBP messages with mixture of Gaussians and formulate their calculation as Monte Carlo integration problems. The new algorithm is general enough to deal with hybrid models that may represent linear or nonlinear equations and arbitrary probability distributions.
- we propose the Hybrid Loopy Belief Propagation algorithm, which extends the Loopy Belief Propagation (LBP) (Murphy et al., 1999) and Nonparametric Belief Propaga- tion (NBP) (Sudderth et al., 2003) algorithms to deal with general hybrid Bayesian networks.
- We now know how to use Monte Carlo inte- gration methods to estimate the LBP messages, represented as sets of weighted samples.
- Monte Carlo and Mixture of Gaussians
- Its accuracy is comparable to the Gibbs sampler in (Sudderth et al., 2003) but much more efficient given the same number of samples

6: DMP
======
- Theoretically, DMP can provide exact results for a polytree CLG
- In this paper, we present a new representation and inference algorithm called Direct Message Passing (DMP) to exchange messages between discrete and continuous variables. Essentially, DMP provides an alternative for probabilistic inference in hybrid Bayesian networks. It can be extended with unscented transformation20 for general hybrid model with nonlinear and/or non-Gaussian distributions.


========
- The variance of a random variable X is its second central moment, the expected value of the squared deviation from the mean μ = E[X]:
http://upload.wikimedia.org/math/4/7/e/47e1dfd328acb38d8b16986de8325245.png

Covariance
==========
- How much two random variables change together
- The covariance between two jointly distributed real-valued random variables x and y with finite second moments is defined[2] as
http://upload.wikimedia.org/math/8/d/9/8d9b71686f79329926f82755b1584d23.png

Deterministic variables with Conditional Linear Gaussian
========================================================
- A variable is conditionally deterministic when it can be described by a deterministic function of a set of direct predecessors. Deterministic means not subject to chance, or certain.
- When a hybrid Bayesian network has conditionally deterministic variables with continuous parents, the joint density function for the continuous variables does not exist. 
- A probability density function (pdf), or density of a continuous random variable, is a function that describes the relative likelihood for this random variable to take on a given value. 
 The probability of the random variable falling within a particular range of values is given by the integral of this variable’s density over that range.
- The joint probability density function is defined as a function of the n variables, such that, for any domain D in the n-dimensional space of the values of the variables X1, …, Xn, the probability that a realisation of the set variables falls inside the domain D is

http://upload.wikimedia.org/math/c/2/f/c2fccc376ef4c9d2f7a4131a29ce75ec.png

- Conditional linear Gaussian distributions can handle such cases when the continuous variables have a multi-variate normal distribution and the discrete variables do not have continuous parents


Mixture Models
==============

- In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data-set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population

- Here the underlying random variables may be random vectors, each having the same dimension, in which case the mixture distribution is a multivariate distribution.

- In cases where each of the underlying random variables is continuous, the outcome variable will also be continuous and its probability density function is sometimes referred to as a mixture density. The cumulative distribution function (and the probability density function if it exists) can be expressed as a convex combination (i.e. a weighted sum, with non-negative weights that sum to 1) of other distribution functions and density functions. The individual distributions that are combined to form the mixture distribution are called the mixture components, and the probabilities (or weights) associated with each component are called the mixture weights.

	http://en.wikipedia.org/wiki/File:Gaussian-mixture-example.svg

Mixtures of Truncated Exponentials (MTE)
========================================
-  Truncated probability distribution is when the value of a random variable is either bounded below or above (or both)
-  Exponential distribution is the probability distribution that describes the time between events in a process in which events occur continuously and independently at a constant average rate
	http://en.wikipedia.org/wiki/File:Mean_exp.svg

	- The probability density function (pdf) of an exponential distribution is
		http://upload.wikimedia.org/math/4/5/0/450b84da3274134a4f8280d8b46067b9.png

	- The mean or expected value of an exponentially distributed random variable X with rate parameter λ is given by
	    http://upload.wikimedia.org/math/a/1/2/a12c0f2b9a3efd768c970f0ddf6535d4.png

	- The variance of X is given by
		http://upload.wikimedia.org/math/d/a/b/dab5a58e08e019e1b28a884a2caa19c2.png

Mixtures of Gaussians (MoG)
===========================
- In mathematics, a Gaussian function (named after Carl Friedrich Gauss) is a function of the form:
http://upload.wikimedia.org/math/1/9/f/19fc9f01844615b08c190fdac5a87eda.png

- The graph of a Gaussian is a characteristic symmetric "bell curve" shape that quickly falls off towards zero. The parameter a is the height of the curve's peak, b is the position of the center of the peak, and c (the standard deviation) controls the width of the "bell".

- In statistics and probability theory, Gaussian functions appear as the density function of the normal distribution, which is a limiting probability distribution of complicated sums, according to the central limit theorem.
	-> In probability theory, the central limit theorem (CLT) states that, given certain conditions, the arithmetic mean of a sufficiently large number of iterates of independent random variables, each with a well-defined expected value and well-defined variance, will be approximately normally distributed.[1]